<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/svg+xml" href="/assets/favicon.svg">
        <link rel="stylesheet" href="/styles/styles.css">
        <title>Nash Equilibria Hide from Algorithms | salm.dev</title>
                <script src="/assets/js/mathjax-config.js"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <link rel="preconnect" href="https://cdn.jsdelivr.net">
        <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" as="script">
            </head>
    <body>
        <div>
            <header>
                <a href="/">home</a> / <a href="/writing/">writing</a>
            </header>
            <article>
                <h1 id="nash-equilibria-hide-from-algorithms">Nash
                Equilibria Hide from Algorithms</h1>
                <div class="description">
                <p>Mathematics proves Nash equilibria must exist in
                every game, but computer science proves we can’t find
                them efficiently. <span class="date-info"><span
                class="date">2025-09-30</span></span></p>
                </div>
                <p>When Uber and Lyft set prices in your neighborhood,
                their algorithms are competing in a game. Lower prices
                attract riders but hurt profits. Higher prices boost
                margins but lose customers. Both pricing algorithms
                update every few minutes, responding to one another.</p>
                <p>This is an example of game theory in the real world,
                with millions of pricing decisions daily, no
                communication, and an equilibrium that must exist, but
                that nobody can compute efficiently.</p>
                <h2 id="whats-a-game">What’s a Game?</h2>
                <p>Every finite game needs three things. Players <span
                class="math inline">\(\mathcal{N} = \{1, 2, ...,
                n\}\)</span> who make decisions. Each player <span
                class="math inline">\(i\)</span> has actions <span
                class="math inline">\(A_i\)</span>—their available
                moves. And payoffs <span class="math inline">\(u_i: A_1
                \times ... \times A_n \to \mathbb{R}\)</span> that map
                every possible combination of choices to a number
                representing how much each player values that
                outcome.</p>
                <p>We can represent Rock-Paper-Scissors as:</p>
                <style>
                .payoff-matrix {
                    width: auto;
                    margin: 2em auto;
                    border-collapse: collapse;
                    border: 1px solid var(--border-color);
                    font-family: var(--font-mono);
                }
                .payoff-matrix th,
                .payoff-matrix td {
                    text-align: center;
                    padding: 0.75em 1.25em;
                    border: 1px solid var(--border-color);
                }
                .payoff-matrix th {
                    background: none;
                    font-weight: normal;
                    font-style: italic;
                }
                .payoff-matrix tr:first-child th {
                    border-bottom: 1px solid var(--text-color);
                }
                .payoff-matrix th:first-child,
                .payoff-matrix td:first-child {
                    font-style: italic;
                    background: none;
                    border-right: 1px solid var(--text-color);
                }
                .payoff-matrix td:not(:first-child) {
                    font-weight: bold;
                }
                </style>
                <table class="payoff-matrix">
                <tr>
                <th>
                </th>
                <th>
                Rock
                </th>
                <th>
                Paper
                </th>
                <th>
                Scissors
                </th>
                </tr>
                <tr>
                <td>
                Rock
                </td>
                <td>
                0,0
                </td>
                <td>
                -1,1
                </td>
                <td>
                1,-1
                </td>
                </tr>
                <tr>
                <td>
                Paper
                </td>
                <td>
                1,-1
                </td>
                <td>
                0,0
                </td>
                <td>
                -1,1
                </td>
                </tr>
                <tr>
                <td>
                Scissors
                </td>
                <td>
                -1,1
                </td>
                <td>
                1,-1
                </td>
                <td>
                0,0
                </td>
                </tr>
                </table>
                <p>The numbers (0, 1, -1) are arbitrary. We could use
                (5, 10, 0) or any values that preserve the ordering.
                Winning beats tying beats losing, and the payoffs just
                encode these preferences.</p>
                <h2 id="pure-vs.-mixed-strategies">Pure vs. Mixed
                Strategies</h2>
                <p>We call a strategy pure if a player chooses a single
                action with certainty. In Rock-Paper-Scissors, “always
                play Rock” is a pure strategy. This works great until
                your opponent notices you’re that predictable. If I
                always play Rock, you’ll play Paper and always win.</p>
                <p>So, what if I randomize instead?</p>
                <p>A mixed strategy assigns probabilities to each pure
                action. Let <span
                class="math inline">\(x_{i,a_i}\)</span> denote the
                probability that player <span
                class="math inline">\(i\)</span> plays action <span
                class="math inline">\(a_i\)</span>.</p>
                <p>In general, where <span
                class="math inline">\(x\)</span> represents the mixed
                strategy profile of all players: <span
                class="math display">\[u_i(x) = \sum_{a_1 \in A_1}
                \cdots \sum_{a_n \in A_n} x_{1,a_1} \cdots x_{n,a_n}
                \cdot u_i(a_1, \ldots, a_n)\]</span></p>
                <p>We calculate the expected payoff by considering every
                possible outcome of the game. For each outcome, we
                multiply the payoff by the probability it occurs (which
                is the product of all players’ individual action
                probabilities), then sum everything up.</p>
                <p>For example, if both players mix uniformly in
                Rock-Paper-Scissors with <span
                class="math inline">\(x_{1,\text{Rock}} =
                x_{1,\text{Paper}} = x_{1,\text{Scissors}} =
                \frac{1}{3}\)</span>, each of the 9 outcomes occurs with
                probability <span
                class="math inline">\(\frac{1}{9}\)</span>. The three
                wins, three losses, and three ties balance perfectly,
                giving expected payoff 0.</p>
                <h2 id="nash-equilibrium">Nash Equilibrium</h2>
                <p>A Nash equilibrium occurs when no player can
                unilaterally improve their payoff.</p>
                <p>Formally, let <span
                class="math inline">\(x_{-i}\)</span> denote strategies
                of all players except <span
                class="math inline">\(i\)</span>, and <span
                class="math inline">\(X_i = \Delta(A_i)\)</span> be the
                set of all probability distributions over player <span
                class="math inline">\(i\)</span>’s actions. A strategy
                profile <span class="math inline">\(x^* = (x_1^*, ...,
                x_n^*)\)</span> is a Nash equilibrium when:</p>
                <p><span class="math display">\[u_i(x_i^*, x_{-i}^*)
                \geq u_i(x_i, x_{-i}^*) \quad \forall x_i \in X_i,
                \forall i \in \mathcal{N}\]</span></p>
                <p>In Rock-Paper-Scissors, the equilibrium has each
                player mixing uniformly: <span
                class="math inline">\(x_1^* = x_2^* = (\frac{1}{3},
                \frac{1}{3}, \frac{1}{3})\)</span>. No player can
                improve by deviating, as any other strategy against
                uniform mixing still yields expected payoff 0.</p>
                <h2 id="cycles">Cycles</h2>
                <p>Game theory assumes players are rational. They
                maximize their payoffs given what others do. This
                suggests a natural algorithm. Start anywhere, have each
                player switch to their best response, then repeat until
                nobody wants to change.</p>
                <p>Say you play only Rock. My best response is to play
                only Paper, winning every round. Once you notice, your
                best response shifts to Scissors. Then I switch to Rock,
                you switch to Paper, and we’re back where we started.
                The cycle repeats forever through pure strategies.</p>
                <p>The same cycling happens with mixed strategies, just
                in a higher-dimensional space. Instead of jumping
                between Rock, Paper, and Scissors, probability
                distributions rotate continuously around the equilibrium
                without converging.</p>
                <p>The best response operator formalizes this. Player
                <span class="math inline">\(i\)</span> chooses the
                strategy maximizing their payoff given everyone else’s
                choices. <span
                class="math display">\[BestResponse_i(x_{-i}) =
                \arg\max_{x_i \in X_i} u_i(x_i, x_{-i})\]</span></p>
                <p>Iterating this operator in Rock-Paper-Scissors loops
                forever. We know an equilibrium exists at <span
                class="math inline">\((1/3, 1/3, 1/3)\)</span>, but best
                response dynamics circle around it.</p>
                <p><img src="images/best_response_cycle.jpeg" alt="Best response cycle diagram" loading="lazy" style="display: block; margin: 0 auto; max-width: 350px; width: 100%; height: auto;"></p>
                <p>In the replicator dynamics below, trajectories orbit
                the Nash equilibrium (red star) without ever reaching
                it. Instead, the oscillations continue indefinitely:</p>
                <p><img src="images/replicator_with_arrows.jpeg" alt="Replicator dynamics showing cycling behavior" loading="lazy" style="display: block; margin: 0 auto; max-width: 700px; width: 100%; height: auto;"></p>
                <h2 id="nashs-existence-proof">Nash’s Existence
                Proof</h2>
                <p>Nash’s insight was to draw from topology, not
                optimization. A fixed point of a function is any input
                that the function doesn’t change: <span
                class="math inline">\(f(x) = x\)</span>. Nash realized
                that if he could construct a function whose fixed points
                are exactly the Nash equilibria, then topology’s fixed
                point theorems would guarantee equilibria exist.</p>
                <p>Nash builds his function using regret: how much
                better player <span class="math inline">\(i\)</span>
                could do by switching to pure action <span
                class="math inline">\(a_i\)</span> while others keep
                their mixed strategies. <span
                class="math display">\[r_{i,a_i}(x) = u_i(a_i, x_{-i}) -
                u_i(x)\]</span> Positive regret means you wish you’d
                played that action.</p>
                <p>Nash constructs a function <span
                class="math inline">\(\phi: X \to X\)</span> where <span
                class="math inline">\(X = X_1 \times \cdots \times
                X_n\)</span> is the product of all players’ mixed
                strategy spaces. This map increases probabilities on
                actions with positive regret and decreases those
                without. Each component adjusts player <span
                class="math inline">\(i\)</span>’s probability of
                playing action <span
                class="math inline">\(a_i\)</span>:</p>
                <p><span class="math display">\[\phi_{i,a_i}(x) =
                \frac{x_{i,a_i} + [r_{i,a_i}(x)]^+}{1 + \sum_{a_i&#39;
                \in A_i} [r_{i,a_i&#39;}(x)]^+}\]</span></p>
                <p>where <span class="math inline">\([\cdot]^+ = \max(0,
                \cdot)\)</span> takes only positive values. The
                numerator adds positive regret to the current
                probability; the denominator renormalizes so
                probabilities still sum to 1.</p>
                <p>At Nash equilibrium, no regrets are positive, so
                <span class="math inline">\(\phi(x^*) = x^*\)</span>
                because nothing changes when there’s nothing to improve.
                Conversely, any fixed point <span
                class="math inline">\(\phi(x^*) = x^*\)</span> must be a
                Nash equilibrium since no player has positive
                regret.</p>
                <p>The function <span
                class="math inline">\(\phi\)</span> maps mixed
                strategies to mixed strategies continuously. <a
                href="https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem">Brouwer’s
                theorem</a> then guarantees a fixed point<a href="#fn1"
                class="footnote-ref" id="fnref1"
                role="doc-noteref"><sup>1</sup></a>. But the theorem is
                non-constructive: it proves equilibria must exist
                without telling us how to find them.</p>
                <p><img src="images/nash_fixed_point.jpeg" alt="Nash's fixed point function visualization" loading="lazy" style="display: block; margin: 0 auto; max-width: 700px; width: 100%; height: auto;"></p>
                <aside id="footnotes"
                class="footnotes footnotes-end-of-section"
                role="doc-endnotes">
                <hr />
                <ol>
                <li id="fn1"><p>Brouwer’s theorem says any continuous
                function from a compact convex set to itself must have a
                fixed point. Since mixed strategies form such a set and
                <span class="math inline">\(\phi\)</span> is continuous,
                a fixed point (Nash equilibrium) must exist. The catch
                is the theorem doesn’t tell us how to find it.<a
                href="#fnref1" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                </ol>
                </aside>
                <h2 id="the-computational-complexity-gap">The
                Computational Complexity Gap</h2>
                <p>We know equilibria exist. Actually finding them
                splits sharply between tractable and intractable
                cases.</p>
                <p>Zero-sum games, where one player’s gain equals the
                other’s loss at every outcome, yield to linear
                programming<a href="#fn2" class="footnote-ref"
                id="fnref2" role="doc-noteref"><sup>2</sup></a>. The
                equilibrium computation reduces to:</p>
                <p><span class="math display">\[
                \begin{array}{ll}
                \text{maximize} &amp; v \\
                \text{subject to} &amp; \sum_{a_1 \in A_1} x_{1,a_1}
                \cdot u_1(a_1, a_2) \geq v \quad \forall a_2 \in A_2 \\
                &amp; \sum_{a_1 \in A_1} x_{1,a_1} = 1 \\
                &amp; x_{1,a_1} \geq 0
                \end{array}
                \]</span></p>
                <p>where <span class="math inline">\(x_{1,a_1}\)</span>
                is player 1’s probability of playing action <span
                class="math inline">\(a_1\)</span>, and <span
                class="math inline">\(u_1(a_1, a_2)\)</span> is the
                payoff for player 1 when playing action <span
                class="math inline">\(a_1\)</span> against player 2’s
                action <span class="math inline">\(a_2\)</span>. This
                linear program solves in polynomial time.</p>
                <p>General-sum games, where players’ interests partially
                align, belong to PPAD-complete<a href="#fn3"
                class="footnote-ref" id="fnref3"
                role="doc-noteref"><sup>3</sup></a>.</p>
                <p>The mixed strategy space grows exponentially with
                actions. You might think we could settle for “close
                enough” solutions, but even <span
                class="math inline">\(\varepsilon\)</span>-approximation
                remains PPAD-complete for <span
                class="math inline">\(\varepsilon =
                1/\text{poly}(n)\)</span><a href="#fn4"
                class="footnote-ref" id="fnref4"
                role="doc-noteref"><sup>4</sup></a>.</p>
                <p>Add an arbitrarily small non-zero-sum component to
                any zero-sum game and the complexity jumps from P to
                PPAD-complete. The slightest cooperation breaks our
                algorithms.</p>
                <p><img src="./images/cycling_vs_convergent.jpeg" alt="Comparison: Cycling vs convergent dynamics" loading="lazy" style="display: block; margin: 0 auto; max-width: 700px; width: 100%; height: auto;"></p>
                <aside id="footnotes"
                class="footnotes footnotes-end-of-section"
                role="doc-endnotes">
                <hr />
                <ol start="2">
                <li id="fn2"><p>Von Neumann’s minimax theorem (1928)
                showed that zero-sum games always have a well-defined
                value where both players’ optimal strategies meet. We
                can use linear programming here because the theorem
                guarantees the LP will find the equilibrium.<a
                href="#fnref2" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn3"><p>PPAD (Polynomial Parity Arguments on
                Directed graphs) is a complexity class for problems we
                know have solutions but can’t find efficiently.
                Daskalakis et al. (2006) proved that computing Nash
                equilibria belongs to this class, meaning we likely
                can’t solve them in polynomial time even though they
                must exist.<a href="#fnref3" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn4"><p>An <span
                class="math inline">\(\varepsilon\)</span>-Nash
                equilibrium allows players to be slightly suboptimal. If
                <span class="math inline">\(\varepsilon = 0.01\)</span>,
                no player could improve their payoff by more than 1% by
                switching strategies. You’d think this relaxation would
                make the problem easier, but it remains PPAD-complete
                for polynomially small values of <span
                class="math inline">\(\varepsilon\)</span>.<a
                href="#fnref4" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                </ol>
                </aside>
                <h2
                id="practical-workarounds-in-different-fields">Practical
                Workarounds in Different Fields</h2>
                <p>We can’t compute exact Nash equilibria, so different
                fields have developed their own workarounds:</p>
                <ol type="1">
                <li><p>ML researchers training GANs face this directly.
                The generator tries to fool the discriminator while the
                discriminator tries to detect fakes, formalized as <span
                class="math inline">\(\min_G \max_D V(D,G)\)</span>.
                Using simultaneous gradient descent-ascent, the dynamics
                may converge to local Nash equilibria or cycle
                indefinitely. In practice, adding learning rate decay
                and spectral normalization improves stability but
                doesn’t guarantee convergence.</p></li>
                <li><p>Economists design mechanisms that avoid
                equilibrium computation entirely. Second-price auctions
                (where the winner pays the second-highest bid) make
                truthful bidding optimal regardless of what others do.
                When each player has an obviously best move like this,
                nobody needs to compute equilibria.</p></li>
                <li><p>Game AI uses computational brute force. AlphaGo
                self-plays millions of games to approximate Nash
                equilibria. For simpler games like <a
                href="../winning-pokemon-showdown">Pokemon Showdown
                1v1</a>, the payoff matrices are small enough to solve
                exactly via linear programming. The price of guaranteed
                convergence is massive computation.</p></li>
                <li><p>Correlated equilibria offer a polynomial-time
                alternative computable via linear programming. Think of
                a traffic light randomly assigning “go” to one
                direction. Following its recommendation is optimal for
                everyone. Unlike Nash equilibria which emerge from
                independent decisions, these need external coordination
                signals.</p></li>
                </ol>
                <h2 id="everywhere-and-nowhere">Everywhere and
                Nowhere</h2>
                <p>Nash’s <a
                href="https://www.jstor.org/stable/1969529">1951
                existence proof</a> using Brouwer’s fixed-point theorem
                takes just two pages. The <a
                href="https://doi.org/10.1137/070699652">2006
                PPAD-completeness proof</a> that finding equilibria is
                computationally hard runs over 100 pages, using a
                reduction from 3-SAT through intermediate problems like
                3D-Sperner and arithmetic circuits.</p>
                <p>Two pages prove equilibria exist in every market
                mechanism, every network protocol, every adversarial
                learning system. A hundred pages prove we can’t find
                them efficiently. The mathematics guarantees they’re
                there, while computer science proves they’re out of
                reach<a href="#fn5" class="footnote-ref" id="fnref5"
                role="doc-noteref"><sup>5</sup></a>.</p>
                <p>Every GAN that refuses to converge, every pricing
                algorithm that oscillates, every market that behaves
                unpredictably is seeking an equilibrium we can’t
                compute. We build workarounds and approximations, but
                the real equilibria remain hidden, determining outcomes
                from spaces too large to search. They exist, as proven.
                We observe their effects daily. We just can’t find
                them.</p>
                <aside id="footnotes"
                class="footnotes footnotes-end-of-section"
                role="doc-endnotes">
                <hr />
                <ol start="5">
                <li id="fn5"><p>Thanks to Prof. Manolis Vlatakis and his
                grad course “Game Theory, Optimization, and Learning” at
                UW-Madison, which I took in my third year of undergrad.
                I had a lot of fun learning all of this.<a
                href="#fnref5" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                </ol>
                </aside>
            </article>
<div class="the-end">~ fin ~</div><div class="similar-writing"><div class="post-navigation"><div class="nav-prev">← Prev: <a href="/writing/cs-data-science-done-right/">CS + Data Science Done Right</a></div></div><div class="related-posts"><h4>Related by Tag:</h4><ul><li><small><a href="/writing/winning-pokemon-showdown/">Winning Pokemon Showdown</a></small></li></ul></div></div>
        </div>
        <footer>
            <p>© 2025 salm.dev</p>
        </footer>
    </body>
</html>
